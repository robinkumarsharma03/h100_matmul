==PROF== Connected to process 2605062 (/gpfs/scratch/bsc28/bsc28207/fast.cu/out/matmul)
==PROF== Profiling "warmupKernel()" - 0: 0%....50%....100% - 12 passes
Configuration 1 16384x16384x16384
KERNEL 0
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 1: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 2: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 3: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 4: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 5: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 6: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 7: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 8: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 9: 0%....50%....100% - 12 passes
==PROF== Profiling "nvjet_tst_192x192_64x4_2x1_v_..." - 10: 0%....50%....100% - 12 passes
Average elapsed time: (0.349890) s, performance: (   25.1) TFLOPS. size: (16384).

==PROF== Disconnected from process 2605062
[2605062] matmul@127.0.0.1
  warmupKernel() (1024, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle         4906
    Memory Throughput                 %        10.96
    DRAM Throughput                   %         0.05
    Duration                         us         3.10
    L1/TEX Cache Throughput           %         0.66
    L2 Cache Throughput               %        12.95
    SM Active Cycles              cycle      1205.52
    Compute (SM) Throughput           %        11.43
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.10
    Issue Slots Busy               %        10.30
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        20.59
    -------------------- ----------- ------------

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       742.27
    Mem Busy                               %        10.96
    Max Bandwidth                          %         5.57
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                       19
    L2 Hit Rate                            %        99.51
    Mem Pipes Busy                         %        11.43
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread         1048576
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                3.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 233 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 25.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.44
    Elapsed Cycles                cycle     26415989
    Memory Throughput                 %        77.44
    DRAM Throughput                   %        77.44
    Duration                         ms        18.35
    L1/TEX Cache Throughput           %        49.97
    L2 Cache Throughput               %        84.53
    SM Active Cycles              cycle  27971455.46
    Compute (SM) Throughput           %        62.64
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.31
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         7.64
    Issued Ipc Active     inst/cycle         0.31
    SM Busy                        %        59.07
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (59.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.26
    Mem Busy                               %        52.99
    Max Bandwidth                          %        77.44
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16782232
    L2 Hit Rate                            %        56.56
    Mem Pipes Busy                         %        36.65
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     24949750
    Memory Throughput                 %        76.97
    DRAM Throughput                   %        76.97
    Duration                         ms        17.15
    L1/TEX Cache Throughput           %        57.71
    L2 Cache Throughput               %        89.09
    SM Active Cycles              cycle  24219618.51
    Compute (SM) Throughput           %        66.45
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.35
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.82
    Issued Ipc Active     inst/cycle         0.35
    SM Busy                        %        68.23
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (68.2%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (9.1%) is Uniform.         
          Comparing the two, the overall pipeline utilization appears to be caused by high-latency instructions. See    
          the Kernel Profiling Guide                                                                                    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload.                                                           

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.26
    Mem Busy                               %        56.21
    Max Bandwidth                          %        76.97
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16781720
    L2 Hit Rate                            %        41.50
    Mem Pipes Busy                         %        38.88
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     31880769
    Memory Throughput                 %        82.78
    DRAM Throughput                   %        82.78
    Duration                         ms        21.93
    L1/TEX Cache Throughput           %        43.11
    L2 Cache Throughput               %        67.89
    SM Active Cycles              cycle  32421711.94
    Compute (SM) Throughput           %        51.99
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.26
    Executed Ipc Elapsed  inst/cycle         0.27
    Issue Slots Busy               %         6.59
    Issued Ipc Active     inst/cycle         0.26
    SM Busy                        %        50.97
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (51.0%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        44.94
    Max Bandwidth                          %        82.78
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16805215
    L2 Hit Rate                            %        40.03
    Mem Pipes Busy                         %        30.42
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     33362719
    Memory Throughput                 %        83.82
    DRAM Throughput                   %        83.82
    Duration                         ms        22.94
    L1/TEX Cache Throughput           %        51.61
    L2 Cache Throughput               %        56.82
    SM Active Cycles              cycle  27086944.30
    Compute (SM) Throughput           %        49.59
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.26
    Issue Slots Busy               %         7.88
    Issued Ipc Active     inst/cycle         0.32
    SM Busy                        %        61.00
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (61.0%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (8.1%) is Uniform.         
          Comparing the two, the overall pipeline utilization appears to be caused by high-latency instructions. See    
          the Kernel Profiling Guide                                                                                    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload.                                                           

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.37
    Mem Busy                               %        44.25
    Max Bandwidth                          %        83.82
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16795250
    L2 Hit Rate                            %        52.50
    Mem Pipes Busy                         %        29.01
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     35531873
    Memory Throughput                 %        83.70
    DRAM Throughput                   %        83.70
    Duration                         ms        24.43
    L1/TEX Cache Throughput           %        42.37
    L2 Cache Throughput               %        64.49
    SM Active Cycles              cycle  32987872.03
    Compute (SM) Throughput           %        46.62
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.26
    Executed Ipc Elapsed  inst/cycle         0.24
    Issue Slots Busy               %         6.48
    Issued Ipc Active     inst/cycle         0.26
    SM Busy                        %        50.09
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (50.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.37
    Mem Busy                               %        42.91
    Max Bandwidth                          %        83.70
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16798069
    L2 Hit Rate                            %        48.69
    Mem Pipes Busy                         %        27.28
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     29743450
    Memory Throughput                 %        80.04
    DRAM Throughput                   %        80.04
    Duration                         ms        20.45
    L1/TEX Cache Throughput           %        48.52
    L2 Cache Throughput               %        71.98
    SM Active Cycles              cycle  28808270.58
    Compute (SM) Throughput           %        55.72
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.30
    Executed Ipc Elapsed  inst/cycle         0.29
    Issue Slots Busy               %         7.41
    Issued Ipc Active     inst/cycle         0.30
    SM Busy                        %        57.36
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (57.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.31
    Mem Busy                               %        47.13
    Max Bandwidth                          %        80.04
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16796340
    L2 Hit Rate                            %        46.86
    Mem Pipes Busy                         %        32.60
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     34971229
    Memory Throughput                 %        84.15
    DRAM Throughput                   %        84.15
    Duration                         ms        24.05
    L1/TEX Cache Throughput           %        40.34
    L2 Cache Throughput               %        62.75
    SM Active Cycles              cycle  34645680.67
    Compute (SM) Throughput           %        47.36
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.25
    Executed Ipc Elapsed  inst/cycle         0.24
    Issue Slots Busy               %         6.17
    Issued Ipc Active     inst/cycle         0.25
    SM Busy                        %        47.69
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (47.7%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.37
    Mem Busy                               %        42.69
    Max Bandwidth                          %        84.15
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16796555
    L2 Hit Rate                            %        55.33
    Mem Pipes Busy                         %        27.71
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     29242598
    Memory Throughput                 %        81.30
    DRAM Throughput                   %        81.30
    Duration                         ms        20.11
    L1/TEX Cache Throughput           %        38.80
    L2 Cache Throughput               %        70.55
    SM Active Cycles              cycle  36021742.72
    Compute (SM) Throughput           %        56.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.24
    Executed Ipc Elapsed  inst/cycle         0.29
    Issue Slots Busy               %         5.93
    Issued Ipc Active     inst/cycle         0.24
    SM Busy                        %        45.87
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (45.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        47.88
    Max Bandwidth                          %        81.30
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16783181
    L2 Hit Rate                            %        53.61
    Mem Pipes Busy                         %        33.12
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     29797683
    Memory Throughput                 %        80.34
    DRAM Throughput                   %        80.34
    Duration                         ms        20.48
    L1/TEX Cache Throughput           %        49.26
    L2 Cache Throughput               %        69.05
    SM Active Cycles              cycle  28373238.69
    Compute (SM) Throughput           %        55.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.30
    Executed Ipc Elapsed  inst/cycle         0.29
    Issue Slots Busy               %         7.53
    Issued Ipc Active     inst/cycle         0.30
    SM Busy                        %        58.24
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (58.2%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.31
    Mem Busy                               %        47.04
    Max Bandwidth                          %        80.34
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16790529
    L2 Hit Rate                            %        56.53
    Mem Pipes Busy                         %        32.54
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

  nvjet_tst_192x192_64x4_2x1_v_bz_coopB_TNN (2, 66, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.45
    Elapsed Cycles                cycle     34070768
    Memory Throughput                 %        83.33
    DRAM Throughput                   %        83.33
    Duration                         ms        23.45
    L1/TEX Cache Throughput           %        61.13
    L2 Cache Throughput               %        56.81
    SM Active Cycles              cycle  22866454.61
    Compute (SM) Throughput           %        48.75
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.37
    Executed Ipc Elapsed  inst/cycle         0.25
    Issue Slots Busy               %         9.34
    Issued Ipc Active     inst/cycle         0.37
    SM Busy                        %        72.26
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (72.3%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (9.6%) is Uniform.         
          Comparing the two, the overall pipeline utilization appears to be caused by high-latency instructions. See    
          the Kernel Profiling Guide                                                                                    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload.                                                           

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        43.11
    Max Bandwidth                          %        83.33
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors                 16780824
    L2 Hit Rate                            %        38.70
    Mem Pipes Busy                         %        28.52
    ---------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          213.20
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread           50688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

